{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttc = pd.read_csv('train_transaction.csv', low_memory=False)\n",
    "tic = pd.read_csv('train_identity.csv', low_memory=False)\n",
    "tstc = pd.read_csv('test_transaction.csv', low_memory=False)\n",
    "tstic = pd.read_csv('test_identity.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    numv = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 \n",
    "    for col in df.columns:\n",
    "        vt = df[col].dtypes\n",
    "        if vt in numv:\n",
    "            max_c = df[col].max()\n",
    "            min_c = df[col].min()\n",
    "            if str(vt)[:3] == 'int':\n",
    "                if min_c > np.iinfo(np.int8).min and max_c < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif min_c > np.iinfo(np.int16).min and max_c < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif min_c > np.iinfo(np.int32).min and max_c < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif min_c > np.iinfo(np.int64).min and max_c < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if min_c > np.finfo(np.float16).min and max_c < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif min_c > np.finfo(np.float32).min and max_c < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif min_c > np.finfo(np.float64).min and max_c < np.finfo(np.float64).max:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttc = reduce_mem_usage(ttc)\n",
    "tic = reduce_mem_usage(tic)\n",
    "tstc = reduce_mem_usage(tstc)\n",
    "tstic = reduce_mem_usage(tstic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(ttc, tic, on = 'TransactionID', how = 'left')\n",
    "test = pd.merge(tstc, tstic, on = 'TransactionID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ttc, tic, tstc, tstic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_data = pd.isnull(train).sum().sort_values(ascending=False)\n",
    "miss_per = (miss_data/len(train))*100\n",
    "missing_data = pd.concat(objs = [miss_data, miss_per], keys = ['Columns','Missing values percentage'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delnullcol(dt):\n",
    "    nullcol = [col for col in dt.columns if dt[col].isnull().sum()/dt.shape[0] >= 0.9]\n",
    "    return nullcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_vals = [col for col in train.columns if train[col].value_counts(dropna = False, normalize = True).values[0] >= 0.9]\n",
    "cols=[]\n",
    "for col in rep_vals:\n",
    "    cols.append(train[col].value_counts(dropna = False).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repcols(dt):\n",
    "    rep_vals = [col for col in dt.columns if dt[col].value_counts(dropna = False, normalize = True).values[0] >= 0.9]\n",
    "    return rep_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useless_cols(dt, exep):\n",
    "    null_cols = delnullcol(dt)\n",
    "    print(\"More than 90% null: \" + str(len(null_cols)))\n",
    "    too_many_repeated = repcols(dt)\n",
    "    print(\"More than 90% repeated value: \" + str(len(too_many_repeated)))\n",
    "    cols_to_drop = list(set(null_cols + too_many_repeated))\n",
    "    cols_to_drop.remove(exep)\n",
    "    return cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = useless_cols(train, 'isFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_Major_values(dt, threshold):\n",
    "    Major_values = []\n",
    "    t=dt.value_counts(dropna = True, normalize = True)\n",
    "    for i in range(len(t)):\n",
    "        if t.values[i] >= threshold:\n",
    "            Major_values.append(t.values[i])\n",
    "    return Major_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_Major_Devices(Major_values,dt):\n",
    "    Major_Devices = []\n",
    "    t = dt.value_counts(dropna = True, normalize = True)\n",
    "    for i in Major_values:\n",
    "        for j in t.items():\n",
    "            if j[1] == i:\n",
    "                Major_Devices.append(j[0])\n",
    "    return Major_Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_plot(Major_Devices,d,dt):\n",
    "    plothis=[]\n",
    "    for i in range(len(Major_Devices)):\n",
    "        plothis.append(d.loc[dt == Major_Devices[i]])\n",
    "    if len(plothis) == 0:\n",
    "        return 10\n",
    "    else:\n",
    "        plothis = pd.concat(objs = [i for i in plothis], axis = 0)\n",
    "        return plothis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = \"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,25))\n",
    "p = sns.barplot(x = 'Columns', y = 'Missing values percentage', data = missing_data)\n",
    "p.set_xticklabels(list(train.columns))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,25))\n",
    "p2 = sns.barplot(x = rep_vals, y = cols)\n",
    "plt.title(\"Columns with most repetetive data\")\n",
    "p2.set(xlabel='Columns', ylabel='Number of replitions')\n",
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amnt = sns.barplot(x = train['isFraud'], y = train['TransactionAmt'], data = train)\n",
    "plt.title(\"Amount V Fraud\")\n",
    "amnt.set_xticklabels(['Not Fraud','Fraud'])\n",
    "amnt.set(xlabel='Transaction Amount')\n",
    "amnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train['ProductCD'], hue='isFraud', data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,7):\n",
    "    mv1 = find_Major_values(train['card'+str(i)], 0.05)\n",
    "    md1 = find_Major_Devices(mv1, train['card'+str(i)])\n",
    "    plothis1 = find_plot(md1, train, train['card'+str(i)])\n",
    "    plt.figure(figsize=(12,5))\n",
    "    if type(plothis1) != int:\n",
    "        p4 = sns.countplot(x = plothis1['card'+str(i)], hue = plothis1['isFraud'], data= plothis1)\n",
    "        plt.title(\"Data analysis of card number \"+str(i))\n",
    "        p4.set(xlabel='card data of card number '+str(i), ylabel='Count')\n",
    "        p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 15):\n",
    "    mv1 = find_Major_values(train['C' + str(i)], 0.05)\n",
    "    md1 = find_Major_Devices(mv1, train['C' + str(i)])\n",
    "    plothis1 = find_plot(md1, train, train['C' + str(i)])\n",
    "    plt.figure(figsize=(12,5))\n",
    "    if type(plothis1) != int:\n",
    "        p4 = sns.countplot(x = plothis1['C' + str(i)], hue = plothis1['isFraud'], data= plothis1)\n",
    "        plt.title(\"Data analysis of C\" + str(i))\n",
    "        p4.set(xlabel='C data of C' + str(i), ylabel='Count')\n",
    "        p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 16):\n",
    "    mv1 = find_Major_values(train['D' + str(i)], 0.05)\n",
    "    md1 = find_Major_Devices(mv1, train['D' + str(i)])\n",
    "    plothis1 = find_plot(md1, train, train['D' + str(i)])\n",
    "    plt.figure(figsize=(12,5))\n",
    "    if type(plothis1) != int:\n",
    "        p4 = sns.countplot(x = plothis1['D' + str(i)], hue = plothis1['isFraud'], data= plothis1)\n",
    "        plt.title(\"Data analysis of D\" + str(i))\n",
    "        p4.set(xlabel='D data of D' + str(i), ylabel='Count')\n",
    "        p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    mv1 = find_Major_values(train['M' + str(i)], 0.05)\n",
    "    md1 = find_Major_Devices(mv1, train['M' + str(i)])\n",
    "    plothis1 = find_plot(md1, train, train['M' + str(i)])\n",
    "    plt.figure(figsize=(12,5))\n",
    "    if type(plothis1) != int:\n",
    "        p4 = sns.countplot(x = plothis1['M' + str(i)], hue = plothis1['isFraud'], data= plothis1)\n",
    "        plt.title(\"Data analysis of M\" + str(i))\n",
    "        p4.set(xlabel='M data of M' + str(i), ylabel='Count')\n",
    "        p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    mv1 = find_Major_values(train['id_0'+str(i)], 0.05)\n",
    "    md1 = find_Major_Devices(mv1, train['id_0'+str(i)])\n",
    "    plothis1 = find_plot(md1, train, train['id_0'+str(i)])\n",
    "    plt.figure(figsize=(12,5))\n",
    "    if type(plothis1) != int:\n",
    "        p4 = sns.countplot(x = plothis1['id_0'+str(i)], hue = plothis1['isFraud'], data= plothis1)\n",
    "        plt.title(\"Data analysis of an id_\"+str(i))\n",
    "        p4.set(xlabel='id data of id_'+str(i), ylabel='Count')\n",
    "        p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10,39):\n",
    "    mv1 = find_Major_values(train['id_'+str(i)], 0.05)\n",
    "    md1 = find_Major_Devices(mv1, train['id_'+str(i)])\n",
    "    plothis1 = find_plot(md1, train, train['id_'+str(i)])\n",
    "    plt.figure(figsize=(12,5))\n",
    "    if type(plothis1) != int:\n",
    "        p4 = sns.countplot(x = plothis1['id_'+str(i)], hue = plothis1['isFraud'], data= plothis1)\n",
    "        plt.title(\"Data analysis of id_\"+str(i))\n",
    "        p4.set(xlabel='id data of id_'+str(i), ylabel='Count')\n",
    "        p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train['DeviceType'], hue='isFraud', data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = find_Major_values(train['DeviceInfo'], 0.1)\n",
    "md = find_Major_Devices(mv, train['DeviceInfo'])\n",
    "plothis = find_plot(md, train, train['DeviceInfo'])\n",
    "\n",
    "p3 = sns.countplot(x = plothis['DeviceInfo'], hue = plothis['isFraud'], data= plothis)\n",
    "plt.title(\"Data analysis of majorly used devices\")\n",
    "p3.set(xlabel='Devices', ylabel='Count')\n",
    "p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace(np.inf,999)\n",
    "test = test.replace(np.inf,999)\n",
    "\n",
    "train['TransactionAmt'] = np.log1p(train['TransactionAmt'])\n",
    "test['TransactionAmt'] = np.log1p(test['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['isFraud']\n",
    "train = pd.get_dummies(train)\n",
    "X_train = train.drop('isFraud', axis=1)\n",
    "X_train = X_train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0\n",
    "scaler = MinMaxScaler()\n",
    "for col in X_train.columns:\n",
    "    a = np.array(X_train[col])\n",
    "    a = a.reshape(-1,1)\n",
    "    X_train[col] = scaler.fit_transform(a)\n",
    "    if q >= 100:\n",
    "        break\n",
    "    else:\n",
    "        q+=1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(x, 'dataset.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(y_train, 'datasety.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = joblib.load('dataset.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = joblib.load('datasety.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "lr.fit(x_train, y_train[:np.shape(x_train)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(lr, x_train, y_train[:np.shape(x_train)[0]], cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, bootstrap=False, max_features=0.33, n_jobs=4)\n",
    "rf.fit(x_train, y_train[:np.shape(x_train)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(lr, x_train, y_train[:np.shape(x_train)[0]], cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
